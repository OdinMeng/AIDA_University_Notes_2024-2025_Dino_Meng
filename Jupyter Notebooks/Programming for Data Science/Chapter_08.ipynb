{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science Summary\n",
    "## Chapter 08 - Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "path = \"../P08 - Large Datasets/data/store_data.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Optimization\n",
    "**GOAL.** Overcome memory limitations with tricks to optimize or limit space usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1 - Change datatypes\n",
    "df['Date'] = pd.to_datetime(df['Date']) # Change to date\n",
    "pd.read_csv(path, parse_dates=['Date']) # Use .read_csv() method instead\n",
    "\n",
    "for column in df.select_dtypes(include=['object']):\n",
    "    df[column] = df[column].astype('category') # Change to categorical\n",
    "\n",
    "df['Retail_price'] = df['Retail_price'].astype('int8') # Reduce Integer Sizes\n",
    "\n",
    "# Alternatively you can use hashmaps to load data and convert in one command\n",
    "hash =  {'ProductFamily_ID' : 'category',\n",
    "        'ProductCategory_ID':'category',\n",
    "        'ProductBrand_ID':'category',\n",
    "        'ProductName_ID':'category',\n",
    "        'ProductPackSKU_ID':'category',\n",
    "        'Point-of-Sale_ID':'category',\n",
    "        'Value_units':'int8',\n",
    "        'Value_price':'float32',\n",
    "        'Unit_Price':'float32',\n",
    "        'Retail_price':'float32',\n",
    "        'Is_Promo':'bool'}\n",
    "pd.read_csv(path, dtype=hash)\n",
    "\n",
    "display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1300547 entries, 0 to 1300546\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   Unnamed: 0          1300547 non-null  int64         \n",
      " 1   ProductFamily_ID    1300547 non-null  category      \n",
      " 2   ProductCategory_ID  1300547 non-null  category      \n",
      " 3   ProductBrand_ID     1300547 non-null  category      \n",
      " 4   ProductName_ID      1300547 non-null  category      \n",
      " 5   ProductPackSKU_ID   1300547 non-null  category      \n",
      " 6   Point-of-Sale_ID    1300547 non-null  category      \n",
      " 7   Date                1300547 non-null  datetime64[ns]\n",
      " 8   Value_units         1300547 non-null  float64       \n",
      " 9   Value_price         1300547 non-null  float64       \n",
      " 10  Unit_Price          1300547 non-null  float64       \n",
      " 11  Retail_price        1300547 non-null  int8          \n",
      " 12  Is_Promo            1300547 non-null  int64         \n",
      "dtypes: category(6), datetime64[ns](1), float64(3), int64(2), int8(1)\n",
      "memory usage: 73.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Remark: To check memory usage in more details, use the memory_usage=\"deep\" option when using .info() method\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2 - Chunking\n",
    "df_chunks = pd.read_csv(\"../P08 - Large Datasets/data/store_data.csv\", chunksize=1000, engine=\"c\") # Load data in chunks of 50 with C engine\n",
    "\n",
    "for chunk in df_chunks:\n",
    "    # Interact with chunk here\n",
    "    break\n",
    "\n",
    "# Alternatively make a list of chunks\n",
    "chunks = []\n",
    "for chunk in df_chunks:\n",
    "    chunks.append(chunk)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3 - Deleting variables when it becomes useless (similar to using malloc's free )\n",
    "df_chunks = pd.read_csv(\"../P08 - Large Datasets/data/store_data.csv\", chunksize=1000, engine=\"c\") # Load data in chunks of 50 with C engine\n",
    "\n",
    "chunks = []\n",
    "for chunk in df_chunks:\n",
    "    chunks.append(chunk)\n",
    "\n",
    "del chunk, df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 566.40 MB\n"
     ]
    }
   ],
   "source": [
    "# Remark - If you want to see a particular variable's memory usage, you have to use the sys module\n",
    "memory_usage_MB = sys.getsizeof(df) / (1024 * 1024) # Convert to MB\n",
    "print(f'Memory usage: {memory_usage_MB:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Optimization\n",
    "Time is money. So we want to reduce our time usage as low as possible, without touching our algorithms at all (i.e. not changing our theoretical time complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1 - Apply .itertuples() method to iterate over rows instead of .iterrows()\n",
    "for row in df.itertuples():\n",
    "    # Apply stuff to rows\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2 - Use NumPy vectorization wherever possible, using the .values attribute\n",
    "df['total_cost'] = df['Value_price'].values * df['Value_units'].values # To access columns convert them to arrays\n",
    "\n",
    "df_numpy = df.to_numpy() # Convert dataframe to array\n",
    "column_dict = {column: i for i, column in enumerate(df.columns)} # Conserve columns and index to preserve interpretability\n",
    "row_dict = {row: i for i, row in enumerate(df.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3 - Use dataframe filtering with standard method or using .loc() method; avoid using .query() method\n",
    "df[df['Value_price'].values > 1000]\n",
    "df.loc[df['Value_price'].values > 1000]\n",
    "\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# Remark - If you want to measure time, you have two methods:\n",
    "%time # Jupyter will automatically measure time and display it\n",
    "\n",
    "start_time = time.time() # Calculate time starting from a certain point\n",
    "pass\n",
    "final_time = time.time() - start_time # Calculate time end to a certain point\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
